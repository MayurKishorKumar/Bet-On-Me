{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb7abbd4-1fef-47a4-8d4b-962e7320072d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb7abbd4-1fef-47a4-8d4b-962e7320072d",
    "outputId": "e84a4bd4-15a8-41d4-bfb5-0b3544ea59af",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in ./.local/lib/python3.8/site-packages (0.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.8/site-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from accelerate) (20.9)\n",
      "Requirement already satisfied: psutil in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from accelerate) (5.8.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.8/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: huggingface-hub in ./.local/lib/python3.8/site-packages (from accelerate) (0.22.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./.local/lib/python3.8/site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: pyyaml in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from packaging>=20.0->accelerate) (2.4.7)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: sympy in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.8)\n",
      "Requirement already satisfied: triton==2.2.0 in ./.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: jinja2 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.11.3)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in ./.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: networkx in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.5)\n",
      "Requirement already satisfied: filelock in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.0.12)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: requests in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from huggingface-hub->accelerate) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from huggingface-hub->accelerate) (4.59.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from networkx->torch>=1.10.0->accelerate) (5.0.6)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from requests->huggingface-hub->accelerate) (2020.12.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /shared/centos7/anaconda3/2021.05/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.2.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple/\n",
      "Requirement already satisfied: bitsandbytes in ./.local/lib/python3.8/site-packages (0.42.0)\n",
      "Requirement already satisfied: scipy in ./.local/lib/python3.8/site-packages (from bitsandbytes) (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in ./.local/lib/python3.8/site-packages (from scipy->bitsandbytes) (1.24.4)\n"
     ]
    }
   ],
   "source": [
    "#!pip install -U flash-attn --no-build-isolation\n",
    "\n",
    "!pip install accelerate\n",
    "!pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "106cddcb-d0bc-4a24-b3c8-8ceffce5fa1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.2.2\n",
      "Uninstalling torch-2.2.2:\n",
      "  Successfully uninstalled torch-2.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59BZadB3cc9c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "59BZadB3cc9c",
    "outputId": "adbc4316-b0b0-4df8-ce4c-20e7f3eddd30"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "#from google.colab import drive\n",
    "\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f3d2b0-efe5-494a-9264-c858d4e35f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "-qVKgpfmiUcF",
   "metadata": {
    "id": "-qVKgpfmiUcF"
   },
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(model_name):\n",
    "    \"\"\"\n",
    "    Load pre-trained language model and tokenizer from Hugging Face.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name or path of the pre-trained model.\n",
    "\n",
    "    Returns:\n",
    "        model (AutoModelForCausalLM): Loaded pre-trained language model.\n",
    "        tokenizer (AutoTokenizer): Loaded tokenizer for the pre-trained model.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True) #  attn_implementation=\"flash_attention_2\",\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"\n",
    "    Perform mean pooling on the token embeddings from the language model output.\n",
    "\n",
    "    Args:\n",
    "        model_output (dict): Output from the pre-trained language model.\n",
    "        attention_mask (torch.Tensor): Attention mask for the input sequences.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Mean-pooled sentence embeddings.\n",
    "    \"\"\"\n",
    "    token_embeddings = model_output['hidden_states'][-1]  # First element contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "def generate_embeddings(model, tokenizer, data, batch_size):\n",
    "    \"\"\"\n",
    "    Generate sentence embeddings for a given dataset using the pre-trained language model.\n",
    "\n",
    "    Args:\n",
    "        model (AutoModelForCausalLM): Loaded pre-trained language model.\n",
    "        tokenizer (AutoTokenizer): Loaded tokenizer for the pre-trained model.\n",
    "        data (pd.DataFrame): Dataset containing input sequences and labels.\n",
    "        batch_size (int): Batch size for processing the data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing sentence embeddings and corresponding labels.\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    X, y = data.drop(['HOME_W'], axis=1), data[[\"HOME_W\"]]\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    for i in range(int(len(X) / batch_size) + 1):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processing batch {i}\")\n",
    "\n",
    "        # Sentences we want sentence embeddings for\n",
    "        sentences = X.values[i * batch_size:(i + 1) * batch_size].reshape((-1,)).tolist()\n",
    "        y_true = y.values[i * batch_size: (i + 1) * batch_size].tolist()\n",
    "\n",
    "        # Tokenize sentences\n",
    "        try:\n",
    "            encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**encoded_input, output_hidden_states=True)\n",
    "\n",
    "        # Perform pooling\n",
    "        sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "        # Normalize embeddings\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "        all_embeddings.extend(sentence_embeddings.cpu().numpy())\n",
    "        all_labels.extend(y_true)\n",
    "\n",
    "    result = pd.DataFrame(all_embeddings)\n",
    "    result[\"HOME_W\"] = all_labels\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length=100):\n",
    "    \"\"\"\n",
    "    Generate text using the pre-trained language model.\n",
    "\n",
    "    Args:\n",
    "        model (AutoModelForCausalLM): Loaded pre-trained language model.\n",
    "        tokenizer (AutoTokenizer): Loaded tokenizer for the pre-trained model.\n",
    "        prompt (str): Input prompt for the language model.\n",
    "        max_length (int): Maximum length of the generated text.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text output from the language model.\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    inputs = tokenizer(prompt, padding=True, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, do_sample=False, max_new_tokens=max_length)\n",
    "\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe80dd1-27ce-4550-9d9c-cc0368b033a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f17b9c-7bbb-49c7-93a4-2a4f2848d111",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "r8QKjDWEjAIi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "0d9250c14bbc4fc58eccc5f6c9511abb",
      "8d9316d8bf244c6297e4540c408909b8",
      "e547464b126a4a85ab8a4a24027261aa",
      "5a57d3b6cc6d4a96984f5922d19cc555",
      "8e8d7f5aa95f40aa9a866606689e4f9a",
      "57eef163f34148ada0c6500434e65305",
      "69cd5b7c187547a9a7e3c655d3f86827",
      "409bba369c43403189528a9e967aaf7c",
      "ac2e3f498469492ea7dcbddf402bfb99",
      "b368c36c808c40eb845f525edb753e24",
      "9f3517c3821841dd89b6742fa4ab72dd"
     ]
    },
    "id": "r8QKjDWEjAIi",
    "outputId": "34099492-6078-404f-f1d7-47d9b309abe6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb8ff38da4d4772b86edff40e17a062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set model and access token\n",
    "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "#access_token = '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'\n",
    "model, tokenizer = load_model_and_tokenizer(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dLE3ihzlAAu_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLE3ihzlAAu_",
    "outputId": "66b956c4-ae1e-4812-bb44-4c65e7a2eff7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: betOnMeLLMDataset_L1.csv\n",
      "Processing batch 0\n",
      "Processing batch 50\n",
      "Processing batch 100\n",
      "Processing batch 150\n",
      "Processing batch 200\n",
      "Processing batch 250\n",
      "Processing batch 300\n",
      "Processing batch 350\n",
      "Processing batch 400\n",
      "Processing batch 450\n",
      "Processing batch 500\n",
      "Processing batch 550\n",
      "Processing batch 600\n",
      "Processing batch 650\n",
      "Embeddings saved to: /home/heminway.r/embeddingsl1_llmembeddings.csv\n",
      "Processing file: betOnMeLLMDataset_L2.csv\n",
      "Processing batch 0\n",
      "Processing batch 50\n",
      "Processing batch 100\n",
      "Processing batch 150\n",
      "Processing batch 200\n",
      "Processing batch 250\n",
      "Processing batch 300\n",
      "Processing batch 350\n",
      "Processing batch 400\n",
      "Processing batch 450\n",
      "Processing batch 500\n",
      "Processing batch 550\n",
      "Processing batch 600\n",
      "Processing batch 650\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings\n",
    "batch_size = 5\n",
    "for j in range(1, 6):\n",
    "    path = f'betOnMeLLMDataset_L{j}.csv'\n",
    "    print(f\"Processing file: {path}\")\n",
    "    df = pd.read_csv(path, index_col=0)\n",
    "    # Use INST tag as used in training by Mistral\n",
    "    df[f'DESCRIPTION_L{j}'] = \"[INST]\" + df[f'DESCRIPTION_L{j}'] + \"[\\INST]\"\n",
    "\n",
    "    embeddings = generate_embeddings(model, tokenizer, df, batch_size)\n",
    "\n",
    "    output_path = f'/home/heminway.r/embeddingsl{j}_llmembeddings.csv'\n",
    "    embeddings.to_csv(output_path, index=False)\n",
    "    print(f\"Embeddings saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kvdpZ2YpjCXa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvdpZ2YpjCXa",
    "outputId": "79165ee4-35c6-442f-b672-b480e776fe88"
   },
   "outputs": [],
   "source": [
    "# generating text\n",
    "path = f'/content/drive/MyDrive/betOnMeLLMDataset_L0.csv'\n",
    "df = pd.read_csv(path, index_col=0)\n",
    "\n",
    "prompt = df.iloc[130][\"DESCRIPTION_L0\"] + \" Who do you think won? Why? Think step-by-step.\"\n",
    "generated_text = generate_text(model, tokenizer, prompt)\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text: {generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d9250c14bbc4fc58eccc5f6c9511abb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8d9316d8bf244c6297e4540c408909b8",
       "IPY_MODEL_e547464b126a4a85ab8a4a24027261aa",
       "IPY_MODEL_5a57d3b6cc6d4a96984f5922d19cc555"
      ],
      "layout": "IPY_MODEL_8e8d7f5aa95f40aa9a866606689e4f9a"
     }
    },
    "409bba369c43403189528a9e967aaf7c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57eef163f34148ada0c6500434e65305": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a57d3b6cc6d4a96984f5922d19cc555": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b368c36c808c40eb845f525edb753e24",
      "placeholder": "​",
      "style": "IPY_MODEL_9f3517c3821841dd89b6742fa4ab72dd",
      "value": " 3/3 [01:09&lt;00:00, 23.02s/it]"
     }
    },
    "69cd5b7c187547a9a7e3c655d3f86827": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d9316d8bf244c6297e4540c408909b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_57eef163f34148ada0c6500434e65305",
      "placeholder": "​",
      "style": "IPY_MODEL_69cd5b7c187547a9a7e3c655d3f86827",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "8e8d7f5aa95f40aa9a866606689e4f9a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f3517c3821841dd89b6742fa4ab72dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ac2e3f498469492ea7dcbddf402bfb99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b368c36c808c40eb845f525edb753e24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e547464b126a4a85ab8a4a24027261aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_409bba369c43403189528a9e967aaf7c",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ac2e3f498469492ea7dcbddf402bfb99",
      "value": 3
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
